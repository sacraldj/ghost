#!/usr/bin/env python3
"""
RSS Parser –¥–ª—è –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π
–ü–∞—Ä—Å–∏—Ç RSS feeds –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ API –∫–ª—é—á–∞—Ö
"""

import requests
import feedparser
from datetime import datetime, timezone
from typing import List, Dict, Any
import logging
from urllib.parse import urlparse
import time

logger = logging.getLogger(__name__)

class RSSParser:
    """–ü–∞—Ä—Å–µ—Ä RSS feeds –¥–ª—è –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'GHOST-News-Engine/1.0 (RSS Parser)'
        })
    
    def parse_rss_feed(self, url: str, max_articles: int = 20) -> List[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏—Ç RSS feed –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π"""
        try:
            logger.info(f"–ü–∞—Ä—Å–∏–º RSS feed: {url}")
            
            # –ü–æ–ª—É—á–∞–µ–º RSS feed
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            # –ü–∞—Ä—Å–∏–º XML
            feed = feedparser.parse(response.content)
            
            if feed.bozo:
                logger.warning(f"RSS feed —Å–æ–¥–µ—Ä–∂–∏—Ç –æ—à–∏–±–∫–∏: {url}")
            
            articles = []
            for entry in feed.entries[:max_articles]:
                try:
                    article = self._parse_entry(entry, url)
                    if article:
                        articles.append(article)
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç–∞—Ç—å–∏: {e}")
                    continue
            
            logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π –∏–∑ {url}")
            return articles
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ RSS feed {url}: {e}")
            return []
    
    def _parse_entry(self, entry, source_url: str) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏—Ç –æ—Ç–¥–µ–ª—å–Ω—É—é —Å—Ç–∞—Ç—å—é –∏–∑ RSS feed"""
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            title = entry.get('title', '').strip()
            if not title:
                return None
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
            description = ''
            if hasattr(entry, 'summary'):
                description = entry.get('summary', '').strip()
            elif hasattr(entry, 'description'):
                description = entry.get('description', '').strip()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫—É
            link = entry.get('link', '')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            published_date = None
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                published_date = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
            elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                published_date = datetime(*entry.updated_parsed[:6], tzinfo=timezone.utc)
            else:
                published_date = datetime.now(timezone.utc)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫ –ø–æ URL
            source_name = self._extract_source_name(source_url)
            
            # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç –Ω–æ–≤–æ—Å—Ç–∏
            article = {
                'title': title,
                'description': description,
                'link': link,
                'published_date': published_date,
                'source_name': source_name,
                'source_url': source_url,
                'type': 'rss',
                'raw_data': entry
            }
            
            return article
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç–∞—Ç—å–∏: {e}")
            return None
    
    def _extract_source_name(self, url: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –∏–∑ URL"""
        try:
            parsed = urlparse(url)
            domain = parsed.netloc.lower()
            
            # –ú–∞–ø–ø–∏–Ω–≥ –¥–æ–º–µ–Ω–æ–≤ –Ω–∞ –Ω–∞–∑–≤–∞–Ω–∏—è
            source_mapping = {
                'www.coindesk.com': 'CoinDesk',
                'cointelegraph.com': 'CoinTelegraph',
                'cryptonews.com': 'CryptoNews',
                'news.bitcoin.com': 'Bitcoin News',
                'coindesk.com': 'CoinDesk'
            }
            
            return source_mapping.get(domain, domain)
            
        except Exception:
            return 'Unknown Source'
    
    def filter_articles_by_keywords(self, articles: List[Dict], keywords: List[str]) -> List[Dict]:
        """–§–∏–ª—å—Ç—Ä—É–µ—Ç —Å—Ç–∞—Ç—å–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        if not keywords:
            return articles
        
        filtered_articles = []
        keywords_lower = [kw.lower() for kw in keywords]
        
        for article in articles:
            title_lower = article.get('title', '').lower()
            description_lower = article.get('description', '').lower()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Å—Ç–∞—Ç—å—è —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ
            if any(kw in title_lower or kw in description_lower for kw in keywords_lower):
                filtered_articles.append(article)
        
        return filtered_articles
    
    def get_multiple_feeds(self, feeds_config: List[Dict]) -> List[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö RSS feeds"""
        all_articles = []
        
        for feed_config in feeds_config:
            try:
                url = feed_config.get('url')
                max_articles = feed_config.get('max_articles', 20)
                keywords = feed_config.get('keywords', [])
                
                if not url:
                    continue
                
                # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç—å–∏
                articles = self.parse_rss_feed(url, max_articles)
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
                if keywords:
                    articles = self.filter_articles_by_keywords(articles, keywords)
                
                all_articles.extend(articles)
                
                # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                time.sleep(1)
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è RSS feed {feed_config}: {e}")
                continue
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ (–Ω–æ–≤—ã–µ —Å–Ω–∞—á–∞–ª–∞)
        all_articles.sort(key=lambda x: x.get('published_date', datetime.min), reverse=True)
        
        return all_articles

def main():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ RSS –ø–∞—Ä—Å–µ—Ä–∞"""
    logging.basicConfig(level=logging.INFO)
    
    parser = RSSParser()
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ RSS feeds
    test_feeds = [
        {
            'url': 'https://www.coindesk.com/arc/outboundfeeds/rss/',
            'max_articles': 5,
            'keywords': ['bitcoin', 'crypto']
        },
        {
            'url': 'https://cointelegraph.com/rss',
            'max_articles': 5,
            'keywords': ['ethereum', 'defi']
        }
    ]
    
    print("üöÄ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ RSS –ø–∞—Ä—Å–µ—Ä–∞...")
    articles = parser.get_multiple_feeds(test_feeds)
    
    print(f"\nüì∞ –ü–æ–ª—É—á–µ–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π:")
    for i, article in enumerate(articles[:10], 1):
        print(f"\n{i}. {article['title']}")
        print(f"   –ò—Å—Ç–æ—á–Ω–∏–∫: {article['source_name']}")
        print(f"   –î–∞—Ç–∞: {article['published_date']}")
        print(f"   –°—Å—ã–ª–∫–∞: {article['link']}")

if __name__ == "__main__":
    main()
